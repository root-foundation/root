# How AGI will change our world

---

- [Activity](#activity)
- [Complexity](#complexity)
- [Example: creating a company](#example-creating-a-company)
- [Similarity](#similarity)
- [AGI eats activities and enables new ones.](#agi-eats-activities-and-enables-new-ones)
- [Local vs. global convergence](#local-vs-global-convergence)
- [Predictions](#predictions)
- [1. Peak outcome grows superexponentially.](#1-peak-outcome-grows-superexponentially)
- [2. Competition grows at least exponentially.](#2-competition-grows-at-least-exponentially)
- [3. Outcomes superexponentially become more extreme across the board.](#3-outcomes-superexponentially-become-more-extreme-across-the-board)
- [4. Number of economically viable activities grows superexponentially.](#4-number-of-economically-viable-activities-grows-superexponentially)
- [5. Rate of displacement of winners accelerates.](#5-rate-of-displacement-of-winners-accelerates)
- [6. Absolute number of winners grows at least exponentially.](#6-absolute-number-of-winners-grows-at-least-exponentially)
- [AGI will expand humanity's horizon](#agi-will-expand-humanitys-horizon)

---

When we ask how AGI will change our world, what we're really asking is how it changes the nature of our _activities_: what will we will find valuable to work on, and how we work on them.

## Activity

Let's start with understanding the nature of an activity itself through an example: drive a car.

Bob is a beginner driver who breaks down the activity of driving a car into _sub-activities_: turn it on, steer, accelerate, change gear, check the mirror, etc.

If a sub-activity is simple enough to do it, Bob just does it. If it's too complex, Bob breaks that sub-activity down further into more sub-activities. For example, Bob breaks "change gear" down into remembering the gear pattern, pressing the clutch, etc.

For Bob, the activity of driving a car is represented as a _recursive tree with falling complexity_: i.e., parent activities are more complex than sub-activities. We can think of [complexity](https://en.wikipedia.org/wiki/Game_complexity) as the number of possibilities (or _moves_) within an activity.

A _leaf_ activity (at the bottom) has a complexity that _can_ be tackled without breaking down into more sub-activities. The root is the activity you _want_ to tackle. And importantly, this _difference_ between the complexity of the activity you _want_ to tackle and the complexity of what you _can_ tackle (without breaking down further) is what _causes_ the tree to be constructed.

## Complexity

At first glance, it's tempting to consider the complexity of the parent activity just the _sum_ of complexities of its sub-activities. This feels natural because of course the parent activity contains the moves of the sub-activities, otherwise sub-activities wouldn't be _sub_ activities.

But, in reality, we see that the parent activity actually contains many moves that _can't_ be categorized neatly into these sub-activities.

For example, when Alice, an expert driver, drives a car, she fluidly makes many moves that don't treat those moves as separate sub-activities. For example, she may glance at the mirror while changing gears while turning the steering wheel while skipping to the next song in her music player. Alice has no need to break the activity of driving a car into sub-activities because she can handle its complexity.

We can also see this in the context of startups: a great founder makes moves that can't neatly be categorized into product, design, engineering, brand, marketing, etc. Their moves exist in a fluid _space_ that contains exponentially more possibilities than the possibilities in any sub-activity.

This means that each activity is _exponentially_ more complex than its sub-activities, so complexity increases _superexponentially_ as we climb the tree. In other words, the space of possible moves doesn’t just grow as we go up the tree, it _accelerates_.

## Example: creating a company

To reason about how AGI will impact our activities, let's take a more relevant example: creating a company that wants to automate agriculture with robots.

This root activity is broken into _sub-activities_:

1. Build product
2. Distribute
3. Raise money
4. Hire
5. ... etc.

And these sub-activities are broken down even further. Build product may have the following sub-activities:

1. Craft spec
2. Design robot
3. Engineer prototype
4. ... etc.

This process of breaking down goes on until _leaves_ such as:

1. Write a specific algorithm.
2. Design component in Figma.
3. ... etc.

We break an activity into sub-activities when we can't tackle it as a whole. This can happen because we lack skill to perform it as a whole as we saw with how a beginner driver can't just "drive". We may also be forced to break an activity into sub-activities due to lack of tools. For example, a great filmmaker who already knows what they want to create may still have to break it down into sub-activities to _implement_ it: e.g., animation, etc. because they lack the tools or resources to abstract that sub-activity away.

## Similarity

The lower you go in the tree, the more we have broken the root activity further and further. Each _break_ creates a separation or difference. This means the further down we go, the more the differences compound — i.e., the further away activities are from their common ancestor, the more different they become.

Which is why when we compare two activities further down in our tree, we will find that they are less similar — i.e., ability transfers less easily across them. The higher up we go, ability transfers more easily between activities because they are closer to their common ancestor (i.e., shorter tree path).

In our example above, "Build product" and "Distribute" are more similar than their respective leaves of their sub-trees: "Design component in Figma" vs. "Set up a sales pipeline in Salesforce". The high level reasoning and intuition required to effectively break down "Build product" and "Distribute" into sub-activities and correctly prioritize them is more _similar_ than the domain specific knowledge you need to operate two different tools.

## AGI eats activities and enables new ones.

We can think of AGI as a advancement that _eats_ activities below a certain complexity threshold — *across the board*. In other words, AGI will eat low complexity activities in _any_ domain: design, product, music, art, engineering, law, etc. It distinguishes by _complexity_, not "domain" or any other form of distinctions that we've created between different kinds of work.

In our example, as AGI advances, it will eat activities lower in the tree: starting at the leaves (e.g., "Design component in Figma", "Write a specific algorithm", etc.) and work upwards.

As this happens, we change the way we break activities into sub-activities — i.e., changing our tree. Therefore, we can also see new sub-activities that have been enabled (that we previously couldn't do). For example, In our company, a new activity (and role) could be managing AI agents that will do to the lower complexity work that we previously had to do ourselves using domain specific tools.

**Thus, As AGI continues to push us upwards, it will _converge_ our activities (and thus our attention) towards our _root_ — a _highly complex_, _singular_ activity.**

## Local vs. global convergence

Our activity (creating an agriculture robotics company) is a sub-activity within a larger activity tree that is rooted in creating _general_ robotics company which contains all the possibilities of our agriculture robotics company along with many many more possibilities. And this larger activity tree itself is a part of a much larger _global_ activity tree: i.e, one that contains all of humanity's activities.

While convergence happens locally: towards the root of our company, it also happens globally. AGI enables greater (and faster) progress higher above the tree. This means that progress in a more complex activity higher in the tree _will make our entire activity irrelevant_ in the same way that AGI _made our local leaves irrelevant_.

From this model of how AGI impacts our activities, we can predict what a post AGI world will look like.

## Predictions

As artificial intelligence advances towards and beyond AGI:

1. Peak outcome grows superexponentially.
2. Competition grows at least exponentially.
3. Outcomes superexponentially become more extreme across the board.
4. Number of economically viable activities grows superexponentially.
5. Rate of displacement of winners accelerates at least exponentially.
6. Absolute number of winners grows at least exponentially.

## 1. Peak outcome grows superexponentially.

We value an activity proportional to the possibilities it enables — i.e., proportional to its _complexity_.

Building Youtube is far more valuable than building a niche, genre-specific, video platform because Youtube captures far more possibilities. Similarly, building a general robotics company that can work for many domains is far more valuable than building a domain specific one because it enables far more possibilities.

Since complexity grows superexponentially as we go up the tree, so will the peak value that can be created (i.e., the peak _outcome_).

## 2. Competition grows at least exponentially.

We have already seen that activities become more similar the higher up we go. Since ability transfers more easily across them, so does competition.

Furthermore, due to better training and resources, there will be many more people capable of competing at more complex activities, amplifying competition further.

## 3. Outcomes superexponentially become more extreme across the board.

Within _any_ activity, at any moment, a _smaller_ percentage of individuals will be responsible for a _greater_ share of economic value created — i.e., a _[power law distribution of outcomes](https://en.wikipedia.org/wiki/Power_law)_. Thus, the median _share_ falls.

Outcomes become more extreme as the following increase:

1. Complexity of an activity.
2. Competition (quantity and quality).
3. Leverage (how fast you can make a move).

We observe this relationship between extreme outcomes and complexity in popular games such as Chess and Go. Go has more extreme outcomes than Chess because it is more [complex](https://en.wikipedia.org/wiki/Game_complexity) — i.e., has more possible states or possibilities).

As competition has improved (more players, better computer assisted training, coaching, etc.) over the years, outcomes have become _even more_ extreme (i.e., ELO spread has increased).

Leverage also matters because the faster you can make a move, the more quickly the underlying extreme outcomes manifest. For example, when two competing startup founders can iterate twice as fast, the difference in their outcomes accelerates — the better one will more quickly outcompete the other.

AGI accelerates all three factors:

1. Complexity increases superexponentially.
2. Competition increases at least exponentially.
3. Leverage (AGI capability) has been increasing exponentially.
   Thus, outcomes will acceleratingly become more extreme.

Extreme outcomes aren't new. Complex activities like building startups and creating content have always had extreme outcomes: the best founders and creators generate massive outcomes while the median founders and creators make nothing. **But, for the first time in human history, outcomes will be extreme in _all_ activities, not only in a few activities**. This is because AGI eats less complex activities _across the board_, instead of just in a few domains.

## 4. Number of economically viable activities grows superexponentially.

**All of humanity's activities until now aren't even a drop of water in the ocean of activities that will emerge.**

All of the activities we see in our world are sub-activities for some highly complex root activity that we may not even be able to see right now.

The number of activities that we conceive will be proportional to ratio of: the complexity of the highest complexity activity we can see (i.e., "root"), and the complexity that we can tackle directly without breaking into sub-activities. Since the complexity of activities grows superexponentially the higher up we go, this ratio will always increase. This ratio will likely increase at least exponentially.

## 5. Rate of displacement of winners accelerates.

Winners remaining winners becomes exponentially more difficult. _Churn_ accelerates.

For winners at the current level of complexity, who are aiming to tackle a parent activity (which will be exponentially more complex):

1. Advantage relative to someone just starting off shrinks rapidly. The complexity of the new activity is so massive that the knowledge you gained in the lower complexity activity increasingly becomes an irrelevant advantage relative to someone starting off with no knowledge.
2. Success becomes a drag because learnings increasingly won't translate to higher complexity activities (and will actually create wrong intuitions). You will increasingly be outcompeted by those who _start_ from above.
3. Pool of competition widens exponentially. Increasingly you will be made irrelevant from places you least expect.

At higher complexities, transferring success from an activity to its parent activity is less about scaling what you already know and more about having a fundamental breakthrough.

For example, scaling McDonald's from one city to the world (a previously high complexity activity from a couple of decades ago) is far more straightforward than scaling a robotics company from one domain (agriculture) to a more general robotics company that can handle many domains (an example of _current_ high complexity activity). You can reason about how to scale McDonald's to many locations. But you cannot _linearly_ reason about how to make a domain specific robot _generally_ useful — this requires re-imagining and big breakthroughs.

Those who will crack the more complex activity that will make current winners obsolete will increasingly have to _start_ directly at the higher complexity activity. Success at a lower complexity activity creates unnecessary drag and distraction because the learnings you gain from that traction become less likely to translate into insights in the higher complexity activities above. For example, OpenAI could not have emerged as an evolution of a domain specific AI winner.

Furthermore, the higher up in the tree we are at, the more similar our activities become — i.e., the more easily you can transfer ability from one activity to another. This greatly widens the pool of competition.

Winners will increasingly face competition from places they least expect. We are seeing early signs, the top content creators and startup founders are starting to find themselves rubbing shoulders fighting for the same attention. Similarly, companies building coding agents are finding themselves competing with companies working on project management platforms (because building and managing are converging).

Eventually, some random biology discovery in a quantum computing lab could wipe out a massive sub-tree of robotics companies by enabling embodied intelligence grounded in biology that can generalize far better than mechanical robots. It will be increasingly difficult to predict and counter such competition.

There will be greater overall quality of competition as quality of training and resources become more democratized. As the peak outcome grows superexponentially, more capable individuals will start by directly attacking activities very high in the tree, shaking up many sub-trees of current winners.

It will be tough to remain a winner when you have no idea where you will be outcompeted from, your pool of competent competitors grows superexponentially, and the prizes for winning grow superexponentially leading to many more competent teams working activities that will make yours irrelevant.

**Therefore, most value will be created by companies that haven't been started, by individuals who haven't been born.**

We are entering a new age of creating value. Successfully tackling highly complex activities requires a very different approach than we're used to. Increasingly, the way the best teams operate will look very weird[^weirdwinner] (and "wrong") to current winners.

[^weirdwinner]: We are entering a new age of creating value. Individuals and teams that will win will operate very differently than they do today. Teams will take years just to discover and frame the right activity to work on before they launch anything because you cannot simply _iterate_ upwards from a lower complexity activity. Roles will be distinguished by complexity not "domain". Teams will completely abandon their current products and customers and jump upwards as soon as they see a glimpse of a higher complexity activity. They will simultaneously execute mass layoffs _and_ go on hiring frenzies at every jump in AGI. They will be paranoid about advancements in completely unrelated activities that may have a common ancestor with theirs higher up in the tree. Most of our current winners will be blindsided.

## 6. Absolute number of winners grows at least exponentially.

Even though the distribution of outcomes will be far more extreme, since the peak outcomes grow exponentially, even _tiny_ wins (relative to the peak wins) will still be _massive_. And since the rate of displacement accelerates, in a given period of time, there will be many more who would have touched the extreme success as well.

## AGI will expand humanity's horizon

A post AGI world will be dynamic. The reign of winners will be increasingly short-lived. We will see rapid advancement across the board. AGI will push humanity to dream bigger and work on more and more complex problems. Problems we consider too complex to solve now will seem like simple tasks to our descendants. Thus, advancing and leveraging AGI will enable us to achieve a degree prosperity and abundance that we can't fathom.

(todo: Bet on humanity: _we can always see more than what we can do_).
But, we are _not_ organized in a way that enables us to take advantage of artificial intelligence. In fact, AGI makes our current systems oppressive — concentrating opportunity in the hands of those who are wealthy, and devastating those without means. To understand why, let's study how our current economic system becomes oppressive in a post AGI world.
